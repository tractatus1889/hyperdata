# Tivari v2: Hyperdata 5% â€” LR=5e-5, 50/50 mix

model: "EleutherAI/pythia-1.4b"
corpus: "data/corpora/tivari_hyperdata_5pct.jsonl"
canonical_dataset: "allenai/c4"
canonical_config: "en"
mix_ratio: 0.5  # 50% synthetic, 50% canonical

max_steps: 3000
batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
warmup_steps: 300
save_steps: 500
max_seq_length: 512

bf16: true
seed: 42
run_name: "pythia-1.4b_tivari_hyperdata_5pct"
